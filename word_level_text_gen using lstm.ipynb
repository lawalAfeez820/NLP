{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word level text gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhGRplLd+AAECeTTsC7qmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawalAfeez820/NLP/blob/main/word_level_text_gen%20using%20lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A WORD LEVEL TEXT GENERATION .\n",
        "\n",
        "## Language model is the probability distribution over a sequence of words.There are wo form of generating text which are character level and word level text generation.Due to the time conputation of character level text generation, i decided to develop an NLP word level text generation.\n",
        "\n",
        "## Text generation works by providing seed sentence and this sentence will be used to generate the next possible word , the generated word is the joined to the initial seed sentence to make for the model to generate another word that follows. Here is a description.\n",
        "\n",
        "#### seed sentence: I will\n",
        "#### with the seed sentence above(I will), next posssible word will be generated say \"go\". The \"go\" will be appended to the initial sentence to create a new seed sentence \"I will go\" . This new sentence will then be used to generated another next posible word. Generally, It is text generation is a reculsive process.\n",
        "\n",
        "## Next Word can be generated using sampling(Picking next word base on the conditional probability distribution).There are various way of sampling, Here, I used greedy search and top_k sampling method.\n",
        "\n",
        "## I used tensorflow data api during the implention for fast processing using(cache and prefecth), I also used keras text preprocessing layer for data cleaning and standadization(lowering the alphabet,removing html tags etc.).A simple LSTM  was used during the model building.\n",
        "\n",
        "## Shortcomings: The model was trained with corpus containing few words due to the limitation of colab so it is possible to generate some unneccessary word.A more robost way of getting best result is by using lstm endoder decoder or transformer which will be implemented soon."
      ],
      "metadata": {
        "id": "rikfCoHqZBGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALoUETkTro8l",
        "outputId": "ceaef43c-6e33-4c42-ea65-e30a53309449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  586k  100  586k    0     0  2197k      0 --:--:-- --:--:-- --:--:-- 2206k\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "!curl -O https://s3.amazonaws.com/text-datasets/nietzsche.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset line by line with tensorflow data api\n",
        "text_data=tf.data.TextLineDataset([\"nietzsche.txt\"])"
      ],
      "metadata": {
        "id": "h2cVnqP4sZDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in text_data.take(7):\n",
        "  print(i.numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtooesDVsrjT",
        "outputId": "f672628f-e4cb-4e53-e9d3-99085b964fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFACE\n",
            "\n",
            "\n",
            "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
            "for suspecting that all philosophers, in so far as they have been\n",
            "dogmatists, have failed to understand women--that the terrible\n",
            "seriousness and clumsy importunity with which they have usually paid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting each line to a list of strngs\n",
        "raw_data=text_data.map(lambda x: tf.strings.split(x))\n"
      ],
      "metadata": {
        "id": "OpSszZ8SuOcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in raw_data.take(5):\n",
        "  print(element.numpy())"
      ],
      "metadata": {
        "id": "Ew_88VbduSBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600e2119-09fd-4686-ab41-c1ef6b29f127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'PREFACE']\n",
            "[]\n",
            "[]\n",
            "[b'SUPPOSING' b'that' b'Truth' b'is' b'a' b'woman--what' b'then?' b'Is'\n",
            " b'there' b'not' b'ground']\n",
            "[b'for' b'suspecting' b'that' b'all' b'philosophers,' b'in' b'so' b'far'\n",
            " b'as' b'they' b'have' b'been']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flattening generatedted list\n",
        "raw_data=raw_data.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
        "for i in raw_data.take(5):\n",
        "  print(i.numpy()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1V_pF32wuyl",
        "outputId": "88f25a8a-9a72-4ca4-ddfb-3714b86b6aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'PREFACE'\n",
            "b'SUPPOSING'\n",
            "b'that'\n",
            "b'Truth'\n",
            "b'is'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the window size fuction on the dataset for the number of sequence to use.\n",
        "#the input_size is the number dependent variable for predicting thenext word.\n",
        "#notice that the first variable inside the window function is input_size+1, the one is due to the next word to be predicted\n",
        "\n",
        "\n",
        "input_size=4\n",
        "\n",
        "sequence_data=raw_data.window(input_size+1,drop_remainder=True)\n",
        "\n",
        "for window in sequence_data.take(3):\n",
        "  print(list(window.as_numpy_iterator()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGFm8JQAw7xH",
        "outputId": "e769ffd3-b343-41e1-e663-ec18e60186ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'PREFACE', b'SUPPOSING', b'that', b'Truth', b'is']\n",
            "[b'a', b'woman--what', b'then?', b'Is', b'there']\n",
            "[b'not', b'ground', b'for', b'suspecting', b'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sequence_data=sequence_data.flat_map(lambda x: x.batch(5))"
      ],
      "metadata": {
        "id": "uoVTvAFR765E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sequence_data.take(5):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgbEqvie8zuA",
        "outputId": "58356438-6c56-486d-b568-28026b15012a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'PREFACE' b'SUPPOSING' b'that' b'Truth' b'is'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'a' b'woman--what' b'then?' b'Is' b'there'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'not' b'ground' b'for' b'suspecting' b'that'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'all' b'philosophers,' b'in' b'so' b'far'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'as' b'they' b'have' b'been' b'dogmatists,'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "REi_jmczDTmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating x nd y variable for model fitting preparation\n",
        "sequence_data=sequence_data.map(lambda x : (x[:-1], x[-1]))\n",
        "\n",
        "x_data=sequence_data.map(lambda x,y:x)\n",
        "\n",
        "y_data=sequence_data.map(lambda x,y:y)"
      ],
      "metadata": {
        "id": "99ii9tGd9THn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q41bp3VVD_RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in x_data.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rjLMOtDmkU",
        "outputId": "337cbaa4-5acf-4eda-e322-387a26d9bf6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'PREFACE' b'SUPPOSING' b'that' b'Truth'], shape=(4,), dtype=string)\n",
            "tf.Tensor([b'a' b'woman--what' b'then?' b'Is'], shape=(4,), dtype=string)\n",
            "tf.Tensor([b'not' b'ground' b'for' b'suspecting'], shape=(4,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "58ELeCnEEAGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshaping the x dataset\n",
        "\n",
        "def convert_string(x: tf.Tensor):\n",
        "  str1=\"\"\n",
        "  for element in x:\n",
        "    str1+=element.numpy().decode(\"utf-8\")+\" \"\n",
        "  str1=tf.convert_to_tensor(str1[:-1])\n",
        "  return str1"
      ],
      "metadata": {
        "id": "ZDhC__U49Twb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data=x_data.map(lambda x: tf.py_function(func=convert_string,inp=[x],Tout=tf.string))"
      ],
      "metadata": {
        "id": "SNC9Y31-9Tz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in x_data.take(5):\n",
        "  print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie0HAsJi9T3V",
        "outputId": "ae9f801d-a086-4bd6-e962-091ea602130c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'PREFACE SUPPOSING that Truth', shape=(), dtype=string)\n",
            "tf.Tensor(b'a woman--what then? Is', shape=(), dtype=string)\n",
            "tf.Tensor(b'not ground for suspecting', shape=(), dtype=string)\n",
            "tf.Tensor(b'all philosophers, in so', shape=(), dtype=string)\n",
            "tf.Tensor(b'as they have been', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data.element_spec,y_data.element_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbhtGzP09T7G",
        "outputId": "c541bc48-6d02-4482-8666-71cacc1ccdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorSpec(shape=<unknown>, dtype=tf.string, name=None) TensorSpec(shape=(), dtype=tf.string, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in x_data.map(lambda x : tf.reshape(x,[1])).take(2):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcyhwXSA9T_S",
        "outputId": "0c85a485-7fe2-4bd4-ec43-afe81c476165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'PREFACE SUPPOSING that Truth'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b'a woman--what then? Is'], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data=x_data.map(lambda x : tf.reshape(x,[1]))"
      ],
      "metadata": {
        "id": "7-LDESy-9UEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the dataset with keras preprocessing layer\n",
        "\n",
        "#Preprocessing the text using preprocessing layer text vectorization\n",
        "\n",
        "import re,string\n",
        "def custom_standadization(text):\n",
        "\n",
        "  lowercase=tf.strings.lower(text)\n",
        "\n",
        "  stripped_html=tf.strings.regex_replace(lowercase,\"<br />\" ,\" \")\n",
        "\n",
        "  stripped_number=tf.strings.regex_replace(stripped_html,\"[\\d-]\",\" \")\n",
        "\n",
        "  stripped_punc=tf.strings.regex_replace(stripped_number,\"[%s]\" % re.escape(string.punctuation),\"\")\n",
        "\n",
        "  return stripped_punc"
      ],
      "metadata": {
        "id": "kRo-dTRm9UIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "vectorizer=TextVectorization(standardize=custom_standadization,\n",
        "                             output_sequence_length=4\n",
        ")"
      ],
      "metadata": {
        "id": "srqYRonO9ULt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the vectorizer\n",
        "\n",
        "vectorizer.adapt(raw_data.batch(batch_size=128))"
      ],
      "metadata": {
        "id": "-kxB2VqYG3iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lenth of the vocabulary\n",
        "len(vectorizer.get_vocabulary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kteYTKCjG3zH",
        "outputId": "8d010ae5-1a4d-4bd1-9dc1-227b18f99238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9903"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that will be aplied to each element in the tensor\n",
        "\n",
        "def vectorize_text(text):\n",
        "  text=tf.expand_dims(text,1)\n",
        "  return tf.squeeze(vectorizer(text))"
      ],
      "metadata": {
        "id": "jKvt1SOQG4A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "coi2z8WjM64n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking some examples of the x_data\n",
        "for i in x_data.take(5):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "9O-XEd0WNKnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b52aa81-b440-4033-81f9-f237f421b596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'PREFACE SUPPOSING that Truth'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b'a woman--what then? Is'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b'not ground for suspecting'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b'all philosophers, in so'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b'as they have been'], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the vectorizer to the train data\n",
        "x_data=x_data.map(vectorize_text)\n",
        "\n",
        "y_data=y_data.map(vectorizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "r8dVp5y9G4YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checking the examples\n",
        "\n",
        "for i in x_data.take(2):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "htFDIMJ9G4kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29244ad-697c-41cf-b83a-71e7fd6c6193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([4041  576    9  119], shape=(4,), dtype=int64)\n",
            "tf.Tensor([  8 147  41 143], shape=(4,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in y_data.take(2):\n",
        "  print(i)\n"
      ],
      "metadata": {
        "id": "md2k3XSzG4vC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c55564-5178-4966-b65e-673014c701e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([7 0 0 0], shape=(4,), dtype=int64)\n",
            "tf.Tensor([40  0  0  0], shape=(4,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only one value is required for the y_data, the transformation below select only the first value of y_data\n",
        "y_data=y_data.map(lambda x: x[:1])\n"
      ],
      "metadata": {
        "id": "47NXGpTkFE2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in y_data.take(2):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR8cyQPCF6te",
        "outputId": "bd10fa9e-f611-446b-8e22-4f17957a6822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([7], shape=(1,), dtype=int64)\n",
            "tf.Tensor([40], shape=(1,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joining both x_data and y_data together for optimization and shape \n",
        "train_ds=tf.data.Dataset.zip((x_data,y_data))\n",
        "print(train_ds.element_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny7SzviKGILl",
        "outputId": "6d11a00c-66a1-439f-9c22-ca95d9ce5cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to fix the shape after zipping\n",
        "def fixed_shape(x,y):\n",
        "  x.set_shape([4])\n",
        "  y.set_shape([1])\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "3dFWHaQAGnCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the fix shape function\n",
        "train_ds=train_ds.map(fixed_shape)\n",
        "print(train_ds.element_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emdvskQRG-fY",
        "outputId": "9703a5b3-0f7d-4d2d-b4a7-95f2af14cedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(1,), dtype=tf.int64, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.cardinality"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slctE3rZS2Le",
        "outputId": "5cac4543-92bc-4d6a-e042-5db50e262761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DatasetV2.cardinality of <MapDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(1,), dtype=tf.int64, name=None))>>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorflow data pipeline for optimizing the process\n",
        "\n",
        "from tensorflow.python.data.ops.dataset_ops import AUTOTUNE\n",
        "autotune=AUTOTUNE\n",
        "train_ds=train_ds.shuffle(512).padded_batch(128,drop_remainder=True).cache().prefetch(autotune)"
      ],
      "metadata": {
        "id": "2QS8LoVG3-av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.data.Dataset.padded_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgVu3xlQYJAf",
        "outputId": "b55ef953-2f42-4f3d-f4a5-ed13bc9ba71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.data.ops.dataset_ops.DatasetV2.padded_batch>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0mA0FQw4oUD",
        "outputId": "6630cf96-00ef-4ab7-bbe6-a6e0633a23c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(128, 4), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(128, 1), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "ajgQUhMHMZ32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NzDl4AWbMY5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM,Input,Embedding,Flatten,Dense,Lambda\n",
        "\n",
        "\n",
        "input=Input(shape=4)\n",
        "embedding=Embedding(len(vectorizer.get_vocabulary()),output_dim=40)(input)\n",
        "\n",
        "encoder=LSTM(units=64,return_sequences=True)(embedding)\n",
        "flatten=Flatten()(encoder)\n",
        "output=Dense(len(vectorizer.get_vocabulary()),activation=\"softmax\")(flatten)\n",
        "\n",
        "model=Model(input,output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jbwcAoIY9qzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d4ff6d-0891-4b61-8797-16f501889de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 4)]               0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 4, 40)             396120    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 4, 64)             26880     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9903)              2545071   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,968,071\n",
            "Trainable params: 2,968,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "DpAKGiPROBOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds,epochs=63,batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8-Um3RuOQZD",
        "outputId": "aa3f963b-35d2-4ece-fa54-4e62a2e7fbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/63\n",
            "154/154 [==============================] - 45s 272ms/step - loss: 7.6089 - accuracy: 0.0596\n",
            "Epoch 2/63\n",
            "154/154 [==============================] - 13s 86ms/step - loss: 6.4888 - accuracy: 0.0616\n",
            "Epoch 3/63\n",
            "154/154 [==============================] - 13s 87ms/step - loss: 6.4024 - accuracy: 0.0616\n",
            "Epoch 4/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 6.3429 - accuracy: 0.0616\n",
            "Epoch 5/63\n",
            "154/154 [==============================] - 25s 159ms/step - loss: 6.2947 - accuracy: 0.0614\n",
            "Epoch 6/63\n",
            "154/154 [==============================] - 18s 117ms/step - loss: 6.2517 - accuracy: 0.0619\n",
            "Epoch 7/63\n",
            "154/154 [==============================] - 18s 116ms/step - loss: 6.2063 - accuracy: 0.0675\n",
            "Epoch 8/63\n",
            "154/154 [==============================] - 20s 127ms/step - loss: 6.1479 - accuracy: 0.0795\n",
            "Epoch 9/63\n",
            "154/154 [==============================] - 20s 130ms/step - loss: 6.0642 - accuracy: 0.0840\n",
            "Epoch 10/63\n",
            "154/154 [==============================] - 23s 148ms/step - loss: 5.9576 - accuracy: 0.0877\n",
            "Epoch 11/63\n",
            "154/154 [==============================] - 13s 88ms/step - loss: 5.8233 - accuracy: 0.0920\n",
            "Epoch 12/63\n",
            "154/154 [==============================] - 21s 134ms/step - loss: 5.6605 - accuracy: 0.0980\n",
            "Epoch 13/63\n",
            "154/154 [==============================] - 19s 121ms/step - loss: 5.4797 - accuracy: 0.1031\n",
            "Epoch 14/63\n",
            "154/154 [==============================] - 22s 146ms/step - loss: 5.2870 - accuracy: 0.1103\n",
            "Epoch 15/63\n",
            "154/154 [==============================] - 19s 124ms/step - loss: 5.0841 - accuracy: 0.1189\n",
            "Epoch 16/63\n",
            "154/154 [==============================] - 22s 145ms/step - loss: 4.8696 - accuracy: 0.1329\n",
            "Epoch 17/63\n",
            "154/154 [==============================] - 20s 130ms/step - loss: 4.6470 - accuracy: 0.1521\n",
            "Epoch 18/63\n",
            "154/154 [==============================] - 20s 132ms/step - loss: 4.4217 - accuracy: 0.1805\n",
            "Epoch 19/63\n",
            "154/154 [==============================] - 14s 92ms/step - loss: 4.1978 - accuracy: 0.2112\n",
            "Epoch 20/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 3.9712 - accuracy: 0.2439\n",
            "Epoch 21/63\n",
            "154/154 [==============================] - 14s 89ms/step - loss: 3.7478 - accuracy: 0.2805\n",
            "Epoch 22/63\n",
            "154/154 [==============================] - 14s 89ms/step - loss: 3.5283 - accuracy: 0.3177\n",
            "Epoch 23/63\n",
            "154/154 [==============================] - 22s 141ms/step - loss: 3.3173 - accuracy: 0.3557\n",
            "Epoch 24/63\n",
            "154/154 [==============================] - 19s 124ms/step - loss: 3.1161 - accuracy: 0.3938\n",
            "Epoch 25/63\n",
            "154/154 [==============================] - 20s 129ms/step - loss: 2.9270 - accuracy: 0.4264\n",
            "Epoch 26/63\n",
            "154/154 [==============================] - 15s 96ms/step - loss: 2.7494 - accuracy: 0.4619\n",
            "Epoch 27/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 2.5838 - accuracy: 0.4916\n",
            "Epoch 28/63\n",
            "154/154 [==============================] - 14s 89ms/step - loss: 2.4293 - accuracy: 0.5187\n",
            "Epoch 29/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 2.2858 - accuracy: 0.5461\n",
            "Epoch 30/63\n",
            "154/154 [==============================] - 15s 99ms/step - loss: 2.1515 - accuracy: 0.5730\n",
            "Epoch 31/63\n",
            "154/154 [==============================] - 22s 144ms/step - loss: 2.0265 - accuracy: 0.5956\n",
            "Epoch 32/63\n",
            "154/154 [==============================] - 20s 133ms/step - loss: 1.9098 - accuracy: 0.6173\n",
            "Epoch 33/63\n",
            "154/154 [==============================] - 20s 127ms/step - loss: 1.8007 - accuracy: 0.6385\n",
            "Epoch 34/63\n",
            "154/154 [==============================] - 20s 132ms/step - loss: 1.6983 - accuracy: 0.6603\n",
            "Epoch 35/63\n",
            "154/154 [==============================] - 19s 126ms/step - loss: 1.6023 - accuracy: 0.6809\n",
            "Epoch 36/63\n",
            "154/154 [==============================] - 17s 108ms/step - loss: 1.5123 - accuracy: 0.6984\n",
            "Epoch 37/63\n",
            "154/154 [==============================] - 16s 104ms/step - loss: 1.4276 - accuracy: 0.7158\n",
            "Epoch 38/63\n",
            "154/154 [==============================] - 17s 111ms/step - loss: 1.3480 - accuracy: 0.7319\n",
            "Epoch 39/63\n",
            "154/154 [==============================] - 20s 133ms/step - loss: 1.2728 - accuracy: 0.7469\n",
            "Epoch 40/63\n",
            "154/154 [==============================] - 20s 133ms/step - loss: 1.2013 - accuracy: 0.7626\n",
            "Epoch 41/63\n",
            "154/154 [==============================] - 19s 120ms/step - loss: 1.1338 - accuracy: 0.7771\n",
            "Epoch 42/63\n",
            "154/154 [==============================] - 20s 132ms/step - loss: 1.0696 - accuracy: 0.7897\n",
            "Epoch 43/63\n",
            "154/154 [==============================] - 19s 124ms/step - loss: 1.0090 - accuracy: 0.8022\n",
            "Epoch 44/63\n",
            "154/154 [==============================] - 20s 129ms/step - loss: 0.9516 - accuracy: 0.8143\n",
            "Epoch 45/63\n",
            "154/154 [==============================] - 21s 137ms/step - loss: 0.8972 - accuracy: 0.8256\n",
            "Epoch 46/63\n",
            "154/154 [==============================] - 16s 103ms/step - loss: 0.8457 - accuracy: 0.8380\n",
            "Epoch 47/63\n",
            "154/154 [==============================] - 19s 122ms/step - loss: 0.7970 - accuracy: 0.8497\n",
            "Epoch 48/63\n",
            "154/154 [==============================] - 19s 126ms/step - loss: 0.7507 - accuracy: 0.8598\n",
            "Epoch 49/63\n",
            "154/154 [==============================] - 20s 133ms/step - loss: 0.7071 - accuracy: 0.8684\n",
            "Epoch 50/63\n",
            "154/154 [==============================] - 14s 89ms/step - loss: 0.6655 - accuracy: 0.8768\n",
            "Epoch 51/63\n",
            "154/154 [==============================] - 13s 87ms/step - loss: 0.6264 - accuracy: 0.8846\n",
            "Epoch 52/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 0.5892 - accuracy: 0.8923\n",
            "Epoch 53/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 0.5541 - accuracy: 0.8987\n",
            "Epoch 54/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 0.5206 - accuracy: 0.9065\n",
            "Epoch 55/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 0.4890 - accuracy: 0.9117\n",
            "Epoch 56/63\n",
            "154/154 [==============================] - 13s 88ms/step - loss: 0.4588 - accuracy: 0.9188\n",
            "Epoch 57/63\n",
            "154/154 [==============================] - 14s 89ms/step - loss: 0.4306 - accuracy: 0.9259\n",
            "Epoch 58/63\n",
            "154/154 [==============================] - 13s 87ms/step - loss: 0.4036 - accuracy: 0.9313\n",
            "Epoch 59/63\n",
            "154/154 [==============================] - 13s 88ms/step - loss: 0.3782 - accuracy: 0.9365\n",
            "Epoch 60/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 0.3541 - accuracy: 0.9417\n",
            "Epoch 61/63\n",
            "154/154 [==============================] - 13s 87ms/step - loss: 0.3314 - accuracy: 0.9472\n",
            "Epoch 62/63\n",
            "154/154 [==============================] - 13s 87ms/step - loss: 0.3099 - accuracy: 0.9509\n",
            "Epoch 63/63\n",
            "154/154 [==============================] - 14s 88ms/step - loss: 0.2897 - accuracy: 0.9548\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2bf349ce50>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(encode_sequence):\n",
        "  decoded_sequence=[]\n",
        "\n",
        "  for token in encode_sequence:\n",
        "    decoded_sequence.append(vectorizer.get_vocabulary()[token])\n",
        "  sequence=\" \".join(decoded_sequence)\n",
        "  return sequence"
      ],
      "metadata": {
        "id": "O8WbpQuxdXUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sequence([2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BLQ7-b-geI71",
        "outputId": "ba83eb1c-84db-460e-a086-2e7ef398d4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the of and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"We are in \"\n",
        "a=vectorizer(text).numpy()\n",
        "a=np.array(a).reshape(1,4)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoqfXsJTjefi",
        "outputId": "e1ebe456-debe-47d3-f666-0bbd6120cce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[29 20  6  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()[np.argmax(model.predict(a))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XSmgBErwfOql",
        "outputId": "9abd97fd-4ce2-4dc9-ac5e-29076bfda984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # SAMPLING METHOD FOR THE TEXT GENERATION\n",
        "\n",
        "def softmax(z):\n",
        "  return np.exp(z)/sum(np.exp(z))\n",
        "\n",
        "def greedy_search(conditional_probability):\n",
        "\n",
        "  return np.argmax(conditional_probability)\n",
        "\n",
        "\n",
        "\n",
        "def top_k_sampling(conditional_probability,k):\n",
        "\n",
        "  top_k_sampling,index=tf.math.top_k(conditional_probability,k=k)\n",
        "\n",
        "  top_k_sampling=np.asarray(top_k_sampling).astype(\"float32\")\n",
        "\n",
        "  top_k_sampling=np.squeeze(top_k_sampling)\n",
        "\n",
        "  index=np.asarray(index).astype(\"int32\")\n",
        "\n",
        "  redistributed_k=softmax(top_k_sampling)\n",
        "\n",
        "  redistributed_k=np.asarray(redistributed_k).astype(\"float32\")\n",
        "\n",
        "  sampled_token=np.random.choice(np.squeeze(index),p=redistributed_k)\n",
        "\n",
        "  return sampled_token"
      ],
      "metadata": {
        "id": "3eGjA5RW9Cw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()[top_k_sampling(model.predict(a),3)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yy2z5Lwqn9Kv",
        "outputId": "bdc351aa-a9e9-4d26-fa6d-5545bcae42c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_text(model,seed,text_size):\n",
        "\n",
        "  count_purpose=seed.split()\n",
        "  list_of_text=seed.split()\n",
        "\n",
        "\n",
        "  for i in range(text_size):\n",
        "\n",
        "    if len(list_of_text)>=4:\n",
        "      \n",
        "\n",
        "      vector=vectorizer(\" \".join(list_of_text[-4:]))\n",
        "\n",
        "      vector=np.array(vector).reshape(1,4)\n",
        "\n",
        "      conditional_probability=model.predict(vector)\n",
        "\n",
        "      output=vectorizer.get_vocabulary()[top_k_sampling(conditional_probability,3)]\n",
        "\n",
        "      list_of_text.append(output)\n",
        "\n",
        "    else:\n",
        "\n",
        "      vector=vectorizer(\" \".join(list_of_text))\n",
        "\n",
        "      vector=np.array(vector).reshape(1,4)\n",
        "\n",
        "      conditional_probability=model.predict(vector)\n",
        "\n",
        "      output=vectorizer.get_vocabulary()[top_k_sampling(conditional_probability,5)]\n",
        "\n",
        "      list_of_text.append(output)\n",
        "\n",
        "  return \" \".join(list_of_text)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FXdmcdiwo9ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AkgQFnF5SfF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRYING OUT SOME SAMPLES"
      ],
      "metadata": {
        "id": "-Z8of0n2Shum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JwDjYjl6SZio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generated text using top_k sampling(decided not to use greedy search)\n",
        "generate_text(model,\"we need to try our luck today\",5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NwjhokUpOfj4",
        "outputId": "dbc85652-d0f5-4a09-f825-2f47d3f42eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we need to try our luck today so it be without been'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZHTTye6cSDm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model,\"let's try something out\",3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MswA_EQvSEOb",
        "outputId": "f182c9c7-53b7-4957-8161-b86837f84527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"let's try something out in a mud\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model,\"let's try something out\",3)"
      ],
      "metadata": {
        "id": "c4-PdGaD_0l6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2579bd53-b91d-4a63-c988-9689d0be55b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"let's try something out for that so\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model,\"i will try and check him when i am back\",3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1b-KdWmvS_rh",
        "outputId": "6a664552-a4cf-40ad-8248-906600cfdb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i will try and check him when i am back to his sympathy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model,\"i will try and check him when i am back\",15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xDv0iCQ2TT1y",
        "outputId": "f021a8e6-04fb-49ed-b8e5-86293801f6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i will try and check him when i am back to will to be oneself of the queen not have be been says for fastidious'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "38EbVy7oTRTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "peIGrhfhTSLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}